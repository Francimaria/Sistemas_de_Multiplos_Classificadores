{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mcs_exe_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hj-sRVnKW5J7",
        "outputId": "08a2858f-0762-424a-b5c3-78591c92fb06"
      },
      "source": [
        "!pip install deslib"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: deslib in /usr/local/lib/python3.6/dist-packages (0.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from deslib) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from deslib) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from deslib) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.19.0->deslib) (1.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXUJkB95p0Az"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zni4KThOqMR_"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "from scipy.io import arff\n",
        "import operator\n",
        "from functools import partial\n",
        "from deslib.util.instance_hardness import kdn_score\n",
        "from deslib.util import diversity\n",
        "from prefit_voting_classifier import PrefitVotingClassifier\n",
        "\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
        "from imblearn.metrics import geometric_mean_score\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn import preprocessing\n",
        "from imblearn.over_sampling import SMOTE"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te_SOueHMyyx"
      },
      "source": [
        "load dataset .. \n",
        "\n",
        "Dois bancos de dados binários e com atributos numéricos do [repositório Promise](http://promise.site.uottawa.ca/SERepository/datasets-page.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypYJmAWkrGDt"
      },
      "source": [
        "def load_data(file_name): \n",
        "\tfull_filepath = file_name \n",
        "\n",
        "\tdata, _ = arff.loadarff(full_filepath) \n",
        "\n",
        "\tdf = pd.DataFrame(data)\n",
        "\tdf.dropna(inplace=True)\n",
        "\tdf[df.columns[-1]] = df.apply(lambda x:  x[df.columns[-1]].decode(), axis=1)\n",
        "\t\t\n",
        "\tlabels = pd.DataFrame(df[df.columns[-1]])\n",
        "\tinstances = df.drop([df.columns[-1]], axis=1)\n",
        "   \n",
        "  # pre-processing \n",
        "\tprint('Original dataset shape %s' % labels.value_counts())\n",
        "\tsm = SMOTE(random_state=42)\n",
        "\tinstances, labels = sm.fit_resample(instances, labels)\n",
        "\tprint('Resampled dataset shape %s' % Counter(labels)) \n",
        "\tinstances = pd.DataFrame(preprocessing.scale(instances))\n",
        "  #replace labels \n",
        "\tlabels = pd.DataFrame(labels)\n",
        "\tlabels = labels.replace([\"false\", \"true\"],[0,1])  \n",
        "\treturn instances,labels"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jzQaRs5NZBi",
        "outputId": "e5c6e47d-b641-4333-d7b7-4c6b69d5b9d5"
      },
      "source": [
        "datasets_filenames = ['cm1.arff', 'kc1.arff']\n",
        "\n",
        "instances ={}\n",
        "labels = {}\n",
        "\n",
        "print(\"load data\")\n",
        "for name in datasets_filenames:\n",
        "  print(name)\n",
        "  instances[name], labels[name] = load_data(name)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "load data\n",
            "cm1.arff\n",
            "Original dataset shape defects\n",
            "false      449\n",
            "true        49\n",
            "dtype: int64\n",
            "Resampled dataset shape Counter({'false': 449, 'true': 449})\n",
            "kc1.arff\n",
            "Original dataset shape defects\n",
            "false      1783\n",
            "true        326\n",
            "dtype: int64\n",
            "Resampled dataset shape Counter({'false': 1783, 'true': 1783})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbPZsjoRYjaR"
      },
      "source": [
        "Suponha três variantes do conjunto de validação:\n",
        "\n",
        "(a) o conjunto original 𝒱; \n",
        "\n",
        "(b) o conjunto 𝒱’ contendo apenas as instâncias difíceis (kDN > 0,5) de 𝒱;\n",
        "\n",
        "(c) o conjunto 𝒱’ contendo apenas as instâncias fáceis (kDN <= 0,5) de 𝒱."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sf8EPQ9Bq4E0"
      },
      "source": [
        "threshold = 0.5\n",
        "\n",
        "kdn_conf = [(\"None\", partial(operator.gt, 2)), (\"Hard\", partial(operator.lt, threshold)), (\"Easy\", partial(operator.gt, threshold))]"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlWB2Z7jdkv_"
      },
      "source": [
        "def _filter_based_hardness(instances, labels, hards, op):\n",
        "\n",
        "  triples = [(instances[i], labels[i], hards[i]) for i in range(len(hards))]\n",
        "\n",
        "  return filter(lambda t: op(t[2]), triples)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LI_-YQfknkFx"
      },
      "source": [
        "def select_val_set(instances, labels, kdn_config, k):\n",
        "  \n",
        "  kdn_scores, neighbors = kdn_score(instances,labels.flatten(), k)\n",
        "\n",
        "  filtered = _filter_based_hardness(instances, labels, kdn_scores, kdn_config)\n",
        "\n",
        "  X_val = []\n",
        "  y_val = []\n",
        "\n",
        "  for t in filtered:\n",
        "    X_val.append(t[0])\n",
        "    y_val.append(t[1])\n",
        "\n",
        "  return np.array(X_val), np.array(y_val)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvaBWu4J8Tmo"
      },
      "source": [
        "**Best first**\n",
        "\n",
        "1. Ordena os classificadores de acordo com o erro\n",
        "\n",
        "2. Inicializa o ensemble com o melhor classificador (menor erro)\n",
        "\n",
        "3. Insere o próximo melhor classificador no ensemble e avalia o erro\n",
        "\n",
        "4. Retorna o ensemble com menor erro\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l52haiYlsYZg"
      },
      "source": [
        "def _order_clfs(pool_clf, validation_instances, validation_labels):\n",
        "\tclfs = pool_clf.estimators_\n",
        "\tclfs_feats = pool_clf.estimators_features_\n",
        "\tpredictions = [clf.predict(validation_instances) for clf in clfs]\n",
        "\terrors = [(1 - accuracy_score(validation_labels, predicted_labels)) for predicted_labels in predictions]\n",
        "\ttriples = [(clfs[i], clfs_feats[i], errors[i]) for i in range(len(errors))]\n",
        "\treturn sorted(triples, key=lambda t: t[2])"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Vo0m9VVstOv"
      },
      "source": [
        "def _find_best_first(triples, validation_instances, validation_labels):\n",
        "\tbest_ensemble_error = 100\n",
        "\tbest_ensemble = None\n",
        "\n",
        "\tcur_clfs = []\n",
        "\tcur_feats = []\n",
        "\tfor triple in triples:\n",
        "\t\tclf, clf_feat, error = triple\n",
        "\t\tcur_clfs.append(clf)\n",
        "\t\tcur_feats.append(clf_feat)\n",
        "\t\tensemble = _get_voting_clf(cur_clfs, cur_feats)\n",
        "\t\tpredicted = ensemble.predict(validation_instances)\n",
        "\t\terror = (1 - accuracy_score(validation_labels, predicted))\n",
        "\n",
        "\t\tif error < best_ensemble_error:\n",
        "\t\t\tbest_ensemble_error = error\n",
        "\t\t\tbest_ensemble = ensemble\n",
        "\n",
        "\treturn best_ensemble"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImloINOqtSpQ"
      },
      "source": [
        "def _get_voting_clf(base_clfs, clfs_feats, weights=None):\n",
        "\tpool_size = len(base_clfs)\n",
        "\tclfs_tuples = [(str(i), base_clfs[i]) for i in range(pool_size)]\n",
        "\tif weights is None:\n",
        "\t\treturn PrefitVotingClassifier(clfs_tuples, clfs_feats, voting = 'hard', weights=None)\n",
        "\telse:\n",
        "\t\treturn PrefitVotingClassifier(clfs_tuples, clfs_feats, voting ='hard', weights=weights)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQkuDkfLqYEz"
      },
      "source": [
        "#from https://github.com/jpedrocm/pool-pruning-experiment/blob/master/code/prefit_voting_classifier.py\n",
        "def _best_first_pruning(pool_clf, validation_instances, validation_labels):\n",
        "\tordered_triples = _order_clfs(pool_clf, validation_instances, \n",
        "\t\t                          validation_labels)\n",
        "\n",
        "\treturn _find_best_first(ordered_triples, validation_instances, \n",
        "\t\t                    validation_labels)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AruorsLww7j5"
      },
      "source": [
        "Medidas de diversidade\n",
        "\n",
        "Disagreement measure\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFPrDOiJ-H0_"
      },
      "source": [
        "def divesity_measure(pool, valid_instances,valid_labels,strategy):\n",
        "  pool_size = len(pool.estimators_)\n",
        "\n",
        "  df_diversity = pd.DataFrame(columns = ['strategy', 'pool_size','disagreement_measure'])\n",
        "\n",
        "  measure_dm = 0\n",
        "\n",
        "  if pool_size <= 1:\n",
        "    new_row = {'strategy':strategy,\n",
        "               'pool_size':pool_size,\n",
        "               'disagreement_measure': measure_dm}\n",
        "    df_diversity = df_diversity.append(new_row, ignore_index=True)\n",
        "    \n",
        "    return df_diversity\n",
        "    \n",
        "  for i in range(pool_size-1):\n",
        "    for j in range(i+1,pool_size):  \n",
        "      y_pred1 =  pool.estimators_[i].predict(valid_instances)\n",
        "      y_pred2 =  pool.estimators_[j].predict(valid_instances)\n",
        "\n",
        "      measure_dm += diversity.disagreement_measure(valid_labels, y_pred1, y_pred2)\n",
        "\n",
        "  new_row = {'strategy':strategy,\n",
        "              'pool_size':pool_size,\n",
        "              'disagreement_measure': (2*measure_dm)/(pool_size*(pool_size-1))}\n",
        "  df_diversity = df_diversity.append(new_row, ignore_index=True)\n",
        "  \n",
        "  return df_diversity"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QxpA0JPscmq"
      },
      "source": [
        "taxa de acerto, AUC, g-mean e f-measure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLuZfylafRE3"
      },
      "source": [
        "K-fold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTV10LnjpK_a",
        "outputId": "952e232a-30e0-450b-dc5c-2d97748ad81b"
      },
      "source": [
        "predictions = {}\n",
        "subpredictions = {}\n",
        "n_folds = 10\n",
        "\n",
        "for ds_name in datasets_filenames: \n",
        "  print(\"Dataset name: \", ds_name)\n",
        "  X = instances[ds_name]\n",
        "  y = labels[ds_name]\n",
        "\n",
        "  predictions[ds_name] = {}\n",
        "  subpredictions[ds_name] = {}\n",
        "\n",
        "  skf = StratifiedKFold(n_splits=n_folds)\n",
        "  k = 5 # select 5 neighborns\n",
        "\n",
        "  # split the dataset into 7 folds to train, 2 to validation, and 1 to test\n",
        "  train_size = 0.70\n",
        "  val_size = 0.20\n",
        "  new_train_size = np.around(train_size / (val_size + train_size), 2)\n",
        "  val_size = 1.0 - new_train_size\n",
        "\n",
        "  for fold, division in enumerate(skf.split(X, y), 1):\n",
        "    train_index, test_index = division[0], division[1] \n",
        "\n",
        "    train, valid = train_test_split(train_index, test_size=val_size)\n",
        "\n",
        "    X_train, X_valid, X_test = X.iloc[train], X.iloc[valid], X.iloc[test_index]\n",
        "    y_train, y_valid, y_test = y.iloc[train], y.iloc[valid], y.iloc[test_index]   \n",
        "\n",
        "    predictions[ds_name][fold] = {}\n",
        "    subpredictions[ds_name][fold] = {}\n",
        "\n",
        "    predictions[ds_name][fold][\"labels\"] = np.array(y_test).tolist()\n",
        "\n",
        "    # Use the KDN to select the validation set\n",
        "    for hardness_type, filter_func in kdn_conf:\n",
        "      X_val, y_val = select_val_set(np.array(X_valid), np.array(y_valid), filter_func, k)      \n",
        "      \n",
        "      clf_pool = BaggingClassifier(base_estimator=Perceptron(), n_estimators=100, random_state=0)\n",
        "      clf_pool.fit(X_train, y_train)\n",
        "\n",
        "      best_first = _best_first_pruning(clf_pool, X_val,y_val)\n",
        "\n",
        "      pool_size = len(best_first.estimators_)\n",
        "\n",
        "      cur_predictions_ = clf_pool.predict(np.array(X_test))\n",
        "      cur_predictions_bf = best_first.predict(np.array(X_test))\n",
        "      \n",
        "      predictions[ds_name][fold][hardness_type] = {}\n",
        "      predictions[ds_name][fold][hardness_type]['None'] = np.array(cur_predictions_).tolist()\n",
        "      predictions[ds_name][fold][hardness_type]['best_first'] = np.array(cur_predictions_bf).tolist()\n",
        "\n",
        "      pool_diversity = divesity_measure(clf_pool, X_val, y_val,'None')\n",
        "      bf_diversity = divesity_measure(best_first, X_val,y_val, 'best_first')\n",
        "\n",
        "      subpredictions[ds_name][fold][hardness_type] = [pool_diversity, bf_diversity]      "
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset name:  cm1.arff\n",
            "Dataset name:  kc1.arff\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgAD8a49sY8m"
      },
      "source": [
        "df_result = pd.DataFrame(columns = ['dataset','estrategia','pool_size','kdn','taxa_acerto','AUC', 'g-mean', 'f-measure','disagreement_measure'])\n",
        "pruned_strategy = ['None','best_first']\n",
        "\n",
        "for ds_name in datasets_filenames:   \n",
        "  for hardness_type, filter_func in kdn_conf:\n",
        "    i = -1\n",
        "    for strategy in pruned_strategy:\n",
        "      i += 1\n",
        "      taxa_acerto = []\n",
        "      auc = []\n",
        "      gmean = []\n",
        "      fmeasure = []\n",
        "      pool_size =[]\n",
        "      disagrement= []\n",
        "      for fold in range(1,(n_folds+1)):\n",
        "        _labels = predictions[ds_name][fold][\"labels\"]\n",
        "        _labels_predict = predictions[ds_name][fold][hardness_type][strategy]\n",
        "\n",
        "        df_data = subpredictions[ds_name][fold][hardness_type]\n",
        "\n",
        "        taxa_acerto.append(accuracy_score(_labels,_labels_predict))\n",
        "        auc.append(roc_auc_score(_labels,_labels_predict))\n",
        "        g = geometric_mean_score(np.array(_labels),np.array(_labels_predict))\n",
        "        gmean.append(g)\n",
        "        fmeasure.append(f1_score(_labels,_labels_predict))\n",
        "        pool_size.append(df_data[i].pool_size[0])\n",
        "        disagrement.append(df_data[i].disagreement_measure[0])\n",
        "      new_row = {'dataset': ds_name,\n",
        "                 'estrategia':strategy,\n",
        "                 'pool_size': \"%d (%0.3f) %d\" % (np.min(pool_size), np.mean(pool_size),np.max(pool_size)),\n",
        "                'kdn':hardness_type,\n",
        "                'taxa_acerto': \"%0.3f (%0.3f)\" % (np.mean(taxa_acerto), np.std(taxa_acerto)),\n",
        "                'AUC': \"%0.3f (%0.3f)\" % (np.mean(auc), np.std(auc)),\n",
        "                'g-mean': \"%0.3f (%0.3f)\" % (np.mean(gmean), np.std(gmean)),\n",
        "                'f-measure': \"%0.3f (%0.3f)\" % (np.mean(fmeasure), np.std(fmeasure)),\n",
        "                'disagreement_measure':\"%0.3f (%0.3f)\" % (np.mean(disagrement), np.std(disagrement))}\n",
        "      df_result = df_result.append(new_row, ignore_index=True)\n",
        "      \n",
        "df_result.to_csv(\"/content/df_result.csv\", index=False)\n",
        "df_result"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
